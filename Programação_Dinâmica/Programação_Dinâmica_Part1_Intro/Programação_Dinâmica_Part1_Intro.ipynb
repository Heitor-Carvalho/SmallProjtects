{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sumário\n",
    "\n",
    "- Estados, Recompensas e ações\n",
    "- Maximizando as recompensas\n",
    "- Funçao valor e policy (politica de decisões)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import cliff\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estados, Recompensas e ações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para inicinarmos a discussção sobre programação dinâmica vamos esclarecer alguns conceitos basicos como estados, recompensas e açoes.\n",
    "\n",
    "- **State**: Conjunto de informações suficientes para definir exatamente o sistema.\n",
    "- **Recompensas**: Ganho ou prejuizo obtido ao transitar de um estado para outro ou de estar em um estado e tomar um determinada ação.\n",
    "- **Ações**: Decição tomada dentre as possívels para provovar uma mudança de estado.\n",
    "\n",
    "Para ilustrar esses conceitos vamos usar o seguinte exemplo de um jogo bem simples, onde o objetico é navegar em um mapa 2D evitando determinados espaços.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "| P | 0 | 0 | 0 | 0 | * | 0 | 0 | 0 | 0 | 0 | 0 | \n",
      "-------------------------------------------------\n",
      "| * | * | * | 0 | 0 | * | 0 | 0 | * | * | 0 | 0 | \n",
      "-------------------------------------------------\n",
      "| * | * | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
      "-------------------------------------------------\n",
      "| * | * | * | * | * | * | * | * | * | * | 0 | G | \n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f191a9a1950>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ROWS  = 4\n",
    "COLS  = 12\n",
    "START = (0, 0)\n",
    "GOAL  = (3, 11)\n",
    "\n",
    "OBSTACLE = [(3, 1), (3, 2), (3, 3), (3, 4), (3, 5), (3, 6), (3, 7), (3,8), (2, 1), (1, 1),\n",
    "            (1, 0), (1, 2), (2, 0), (3, 0), (0, 5), (1, 8), (3, 9), (1, 9), (1, 5)]\n",
    "\n",
    "cliff_game = cliff.Cliff(START, GOAL, ROWS, COLS, OBSTACLE)\n",
    "cliff_game.show()\n",
    "\n",
    "\n",
    "rewards = np.zeros((ROWS, COLS))\n",
    "for i in range(0, ROWS):\n",
    "    for j in range(0, COLS):\n",
    "        rewards[i, j] = cliff_game.getReward((i,j))\n",
    "        \n",
    "plt.imshow(rewards/np.max(rewards))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O jogo consiste em começar nas coordenadas (0,0) e ir  ate o ponto em amarelo (3, 11). Os pontos azul escuro determinam pontos de passagem onde ha um custo maior para se mover, de modo que o melhor caminho será obitdo evitando esses pontos.\n",
    "\n",
    "Neste jogo o estado pode ser definido simplesmente pela coordenada do estado presente ou seja os estados possíveis são:\n",
    "\n",
    "s $\\epsilon$ S = {(i, j) in [[0 , 3], [0, 11]]}\n",
    "\n",
    "As ações possívels podem ser:\n",
    "\n",
    "a $\\epsilon$ A = {\"up\", \"down\", \"left\", \"right\"}\n",
    "\n",
    "Com exceções nas bordas, pois não é possível sair para forar do mapa.\n",
    "\n",
    "As recompensas pare este jogo são definidas como:\n",
    "\n",
    "$R(S_t, a_t) = R(S_t, S_{t+1}) = R(S_t)$\n",
    "\n",
    "Ou seja, as recompesas só dependem do estado em que estamos no instante $t$ e são dados por:\n",
    "\n",
    "- 0 - Se estamos no estado inicial\n",
    "- -2 - Se estamos num estado comum (indicado por 0)\n",
    "- -1000 Se estamos num estado marcado por *\n",
    "- +1000 - Se estamos nos estado final (indicado por G)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolhendo uma políica - maximizando recompensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na proximação dinâmica agimos sobre o sistema de acordo com uma politica, ou regra de controle.  Encontrar uma política consiste em encontrar uma mapeamente entre estado e ação para cada estado possível.\n",
    "\n",
    "$\\pi(S_t) = a_{\\pi}$,  para cada $S_t$ $\\epsilon$ S\n",
    "\n",
    "Normalmente não estamos interessados em qualquer politica, mas sim na politica que maximiza nossas recompensas ao longo do caminho. Esse objetivo pode ser expresso através da seguinte equação:\n",
    "\n",
    "$ J(\\pi(S)) = \\Sigma_{0}^{T}\\gamma^tR(S_t, a_\\pi)$\n",
    "\n",
    "$max\\{J(S)\\} = max\\{\\Sigma_{0}^{T}\\gamma^tR(S_t, a_t)\\}$\n",
    "\n",
    "Com isso, temos o problema de otimização de encontrar a politica que maximiza a função custo acima dada as restrições de transição de estados do sistema, a função recompensa e a ações possíveis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo, podemos avaliar a função custo politica que não ótima onde damos 11 passos para a esquerda e 3 para baixo. Esta política nos leva ao objetivo final do trajeto, porém não otimiza a função custo ao passar pelo espaço marcado por *."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "| S | 0 | 0 | 0 | 0 | * | 0 | 0 | 0 | 0 | 0 | 0 | \n",
      "-------------------------------------------------\n",
      "| * | * | * | 0 | 0 | * | 0 | 0 | * | * | 0 | 0 | \n",
      "-------------------------------------------------\n",
      "| * | * | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | \n",
      "-------------------------------------------------\n",
      "| * | * | * | * | * | * | * | * | * | * | 0 | P | \n",
      "-------------------------------------------------\n",
      "Total Reward: 1000\n"
     ]
    }
   ],
   "source": [
    "cliff_game.reset()\n",
    "total_reward = 0\n",
    "for i in range(0, 11):\n",
    "    _, reward = cliff_game.transition(\"right\")\n",
    "    total_reward += reward\n",
    "    cliff_game.show()\n",
    "    time.sleep(0.25)\n",
    "    clear_output(True)\n",
    "for i in range(0, 3):\n",
    "    _, reward = cliff_game.transition(\"down\")\n",
    "    total_reward += reward\n",
    "    cliff_game.show()\n",
    "    time.sleep(0.25)\n",
    "    if i != 2:\n",
    "      clear_output(True)\n",
    "\n",
    "print(\"Total Reward: {}\".format(reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funçao valor e a equação de Bellman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos simplificar o problema de otimização acima pensando na decisão a ser tomada a cada passo. Para iso, definimos a função valor como:\n",
    "\n",
    "$V(S_t) = max\\{\\Sigma_{t}^{T}\\gamma^tR(S_t)\\}$\n",
    "\n",
    "Ou seja, a sometória das recompensas do estado atual até o estado final seguindo uma determinada política, a partir dessa definição podemos relacionar o a função valor de um estado no instante $t$ com o instante $t+1$.\n",
    "\n",
    "$V_t(S_t) = max(R(S_t) + \\gamma V_{t+1}(S_{t+1}))$ - Bellman equation\n",
    "\n",
    "E política otíma pode ser encontrar a partir da função valor através de:\n",
    "\n",
    "$\\pi^* = argmax_{a_t}{(R(S_t) + \\gamma V_{t+1}(S_{t+1}))}$\n",
    "\n",
    "\n",
    "Vemos na equação que a função valor de um estado depende da função valor do estado futuro, o que dificulta a solução do problema de otimização e cálculo da política otíma. \n",
    "\n",
    "\n",
    "Nas próximas partes veremos diferentes formas de resolver esse problema para o exemplo apresentado e algumas variantes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
